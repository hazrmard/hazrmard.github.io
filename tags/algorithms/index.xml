<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Algorithms on Ibrahim Ahmed</title><link>https://iahmed.me/tags/algorithms/</link><description>Recent content in Algorithms on Ibrahim Ahmed</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Ibrahim Ahmed</copyright><lastBuildDate>Mon, 06 Feb 2017 18:47:26 -0600</lastBuildDate><atom:link href="https://iahmed.me/tags/algorithms/index.xml" rel="self" type="application/rss+xml"/><item><title>Algorithms: Balancing</title><link>https://iahmed.me/post/notes/algorithms-balancing/</link><pubDate>Mon, 06 Feb 2017 18:47:26 -0600</pubDate><guid>https://iahmed.me/post/notes/algorithms-balancing/</guid><description>Balancing in algorithms refers to minimizing the complexity of an algorithm by making sure that its constituent parts share the load efficiently. It is not a technique for solving problems. Instead it helps us understand how an existing solution may be optimized.
The theory of balancing Say there is a problem of size \(n\). The problem is such that it can be broken down into a sequence of smaller problems. There are many ways the problem can be broken down:</description></item><item><title>A case study in choosing algorithms</title><link>https://iahmed.me/post/2016-08-13-a-case-study-in-choosing-algorithms/</link><pubDate>Sun, 14 Aug 2016 03:29:23 +0000</pubDate><guid>https://iahmed.me/post/2016-08-13-a-case-study-in-choosing-algorithms/</guid><description>&lt;p>This past year, I have been crunching data from &lt;a href="https://github.com/hazrmard/DarkMatterHalos">dark matter simulations&lt;/a>. Data size can get pretty large when it comes to scientific computing. As I write this post, I have a script running on 3.8 TB (that’s right – 3,700 gigabytes) of cosmic particles. At these levels one starts thinking about parallelizing computations. And therein lay my dilemma and a soon to be learned lesson.&lt;/p></description></item></channel></rss>